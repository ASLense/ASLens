{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MR1g0tdwiJ1l"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import string\n",
    "import h5py\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHRVQXhliM_W"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ULWdW_gEAuYt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Get gpt2\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel,GPT2Config\n",
    "gptconfig = GPT2Config.from_pretrained(\"gpt2\",add_cross_attention=True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token\n",
    "tokenizer.add_special_tokens({\"bos_token\":\"<s>\",\"eos_token\":\"</s>\"})\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\",config=gptconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eArkqT_kiaEd"
   },
   "source": [
    "## Dataset and Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mlv6TGlmiiHh"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XhX_wbBfjBAS",
    "outputId": "76b94508-56c4-4365-de0e-1f8b727ac89c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "bUPQME4Mmr9-",
    "outputId": "92fe3040-9eb4-4f53-be52-dcaf3c89cdc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name       g0yUlOaqL6k\n",
      "landmarks    g0yUlOaqL6k.h5\n",
      "frames                 2026\n",
      "sentences                24\n",
      "Name: 121, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>landmarks</th>\n",
       "      <th>frames</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FzmL8SL6Bow</td>\n",
       "      <td>FzmL8SL6Bow.h5</td>\n",
       "      <td>1196</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FZrU_mEryAs</td>\n",
       "      <td>FZrU_mEryAs.h5</td>\n",
       "      <td>1213</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-g45vqccdzI</td>\n",
       "      <td>-g45vqccdzI.h5</td>\n",
       "      <td>1332</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FzUdcaxw_vs</td>\n",
       "      <td>FzUdcaxw_vs.h5</td>\n",
       "      <td>1826</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-g0iPSnQt6w</td>\n",
       "      <td>-g0iPSnQt6w.h5</td>\n",
       "      <td>1657</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     file_name       landmarks  frames  sentences\n",
       "0  FzmL8SL6Bow  FzmL8SL6Bow.h5    1196          4\n",
       "1  FZrU_mEryAs  FZrU_mEryAs.h5    1213          7\n",
       "2  -g45vqccdzI  -g45vqccdzI.h5    1332         10\n",
       "3  FzUdcaxw_vs  FzUdcaxw_vs.h5    1826         19\n",
       "4  -g0iPSnQt6w  -g0iPSnQt6w.h5    1657         17"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"how2sign.csv\",sep=\"\\t\") # Load how2Sign dataset\n",
    "df.tail()\n",
    "asl_df=pd.read_csv(\"ASLens-landmarks.csv\") # Load ASLlens-landmarks dataset\n",
    "asl_df.drop(121,inplace=True)\n",
    "asl_df.index = np.arange(0,len(asl_df))\n",
    "asl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "hytJwAhxjeVL"
   },
   "outputs": [],
   "source": [
    "class ASLensDataset(Dataset):\n",
    "  def __init__(self, df, asl_df, tokenizer=None, seq_len=90):\n",
    "    self.tokenizer=tokenizer\n",
    "    self.df=df\n",
    "    self.asl_df=asl_df\n",
    "    self.seq_len=seq_len\n",
    "  def __len__(self):\n",
    "    return self.asl_df['sentences'].sum()\n",
    "\n",
    "  def extract_number(self,sentence_id):\n",
    "    # Extract the numeric part after the last underscore\n",
    "    match = re.search(r'_(\\d+)$', sentence_id)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "  def extractFrames(self,ex,index):\n",
    "    fName=self.asl_df[\"landmarks\"][index] # Get file name\n",
    "    file=h5py.File(f\"landmarks/{fName}\") # Load h5 file\n",
    "    # Determine start and end frame\n",
    "    start_frame=int(ex[\"START_REALIGNED\"]*15) \n",
    "    end_frame=int(ex[\"END_REALIGNED\"]*15)\n",
    "    \n",
    "    # Extract each component of the landmarks and concatenate \n",
    "    hand_left=file[\"handLeft\"][start_frame:end_frame]\n",
    "    hand_right=file[\"handRight\"][start_frame:end_frame]\n",
    "    face_lips=file[\"faceLips\"][start_frame:end_frame]\n",
    "    face_oval=file[\"faceOval\"][start_frame:end_frame]\n",
    "    file.close()\n",
    "    x=np.concatenate([hand_left,hand_right,face_lips,face_oval],axis=1)\n",
    "    \n",
    "    return torch.tensor(x)\n",
    "\n",
    "  def __getitem__ (self, idx):\n",
    "    # Determine valid dataframe index from idx\n",
    "    index=0\n",
    "    while idx>self.asl_df['sentences'][index]-1: \n",
    "      idx-=self.asl_df['sentences'][index]\n",
    "      index+=1\n",
    "    file_name=self.asl_df[\"file_name\"][index]\n",
    "    \n",
    "    # Remove sufficient - from file_name\n",
    "    if file_name[-1]==\"-\":\n",
    "      file_name=file_name[:-1]\n",
    "    \n",
    "    # Get values dataframe\n",
    "    sent=df[df['VIDEO_ID']==file_name]\n",
    "    sent = sent.copy()  # Explicit copy\n",
    "    sent['SENTENCE_NUM'] = sent['SENTENCE_ID'].apply(self.extract_number)\n",
    "    sent = sent.sort_values([\"VIDEO_NAME\",\"SENTENCE_NUM\"])\n",
    "    \n",
    "    # Remove duplicates if there are any\n",
    "    if len(sent[sent.duplicated(\"SENTENCE_ID\")])>0:\n",
    "      first = sent.drop_duplicates(subset=\"SENTENCE_NUM\",keep=\"first\")\n",
    "      last = sent.drop_duplicates(subset=\"SENTENCE_NUM\",keep=\"last\")\n",
    "      numOfFrames=self.asl_df[\"frames\"][index]\n",
    "      if numOfFrames>first[\"END_REALIGNED\"].max()*15:\n",
    "        sent = first\n",
    "      else:\n",
    "        sent = last\n",
    "\n",
    "    \n",
    "    ex=sent.iloc[idx] # Get data from asl_df dataframe\n",
    "    \n",
    "    frames=self.extractFrames(ex,index) # Extract franes\n",
    "    frames=frames.type(torch.float32) # Convert to tensor\n",
    "    text=ex[\"SENTENCE\"] # Get text\n",
    "    if len(text)<4: # If length of text is less than 4 add ' ' to text\n",
    "        text+=' '\n",
    "    if self.tokenizer:\n",
    "        # Tokenize text using gpt2 tokenizer\n",
    "        inputs = self.tokenizer(text[:int(len(text)*0.33)], return_tensors=\"pt\", padding=\"max_length\", max_length=25, truncation=True)\n",
    "        labels = self.tokenizer(text[int(len(text)*0.33):], return_tensors=\"pt\", padding=\"max_length\", max_length=25, truncation=True).input_ids\n",
    "    return frames, inputs.input_ids.squeeze(0),inputs.attention_mask.squeeze(0), labels.squeeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "fcC1Fg5KBV1f"
   },
   "outputs": [],
   "source": [
    "# Train test split\n",
    "\n",
    "test_size = 0.15\n",
    "train_df = asl_df.iloc[:int(len(asl_df)*(1-test_size))]\n",
    "test_df = asl_df.iloc[int(len(asl_df)*(1-test_size)):]\n",
    "train_df.index = np.arange(0,len(train_df))\n",
    "test_df.index = np.arange(0,len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "R74ES363BaYk"
   },
   "outputs": [],
   "source": [
    "# Initialize datasets and data loaders\n",
    "\n",
    "train_asl_dataset = ASLensDataset(df,train_df,tokenizer=tokenizer)\n",
    "test_asl_dataset = ASLensDataset(df,test_df,tokenizer=tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_asl_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_asl_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BlnQHZKg1PNL",
    "outputId": "92f9e4f8-6374-46fd-b7b5-d0006dd42cb0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_asl_dataset.__getitem__(211)[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "PBKUV71Gz3g5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25])\n"
     ]
    }
   ],
   "source": [
    "for data,x,m,y in train_loader:\n",
    "  print(x.shape)\n",
    "  break\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "9Hr8-hFGuqPY"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    hidden_size: int\n",
    "    num_layers: int\n",
    "    dropout_rate:  float =0.1\n",
    "    learning_rate:float= 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "tiHqFKyZ44DA"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ASLensEncoder(nn.Module):\n",
    "  def __init__(self,config):\n",
    "    super(ASLensEncoder,self).__init__()\n",
    "    self.config=config\n",
    "    self.conv1 = nn.Sequential(\n",
    "      nn.Conv1d(3, 16, kernel_size=3, padding=1),  # preserves (90, 3)\n",
    "      nn.ReLU(),\n",
    "      nn.Conv1d(16, 32, kernel_size=2,padding=1),                 # reduces width\n",
    "      nn.ReLU(),\n",
    "      nn.Conv1d(32, 64, kernel_size=2,padding=1),                 # reduces width\n",
    "      nn.ReLU(),\n",
    "        )\n",
    "    self.lstm= nn.LSTM(input_size=98,\n",
    "                       hidden_size=self.config.hidden_size,\n",
    "                       num_layers=config.num_layers,\n",
    "                       dropout=config.dropout_rate,\n",
    "                       batch_first=True)\n",
    "\n",
    "\n",
    "  def forward(self,x):\n",
    "    time = x.shape[2] # Get time size\n",
    "    x=x.view(-1, 98,3) # Reshape data tensor\n",
    "    x=x.permute(1,2, 0)   # Set 98 as first \n",
    "    out = self.conv1(x)  # Go through conv1D\n",
    "    out=out.reshape(1, -1, 98).contiguous() # Reshape and use 98 as input_dim to LSTM\n",
    "    out,hidden = self.lstm(out)\n",
    "    return out,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SqkqCy1pjRAk",
    "outputId": "da76261d-b66c-4471-f344-40a452106d90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ASLensEncoder(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv1d(3, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): Conv1d(16, 32, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "    (3): ReLU()\n",
       "    (4): Conv1d(32, 64, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (lstm): LSTM(98, 128, num_layers=2, batch_first=True, dropout=0.2)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ModelConfig(hidden_size=128,num_layers=2,dropout_rate=0.2)\n",
    "encoder = ASLensEncoder(config)\n",
    "encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "_CLjECvv5NpK"
   },
   "outputs": [],
   "source": [
    "for x,y,m,z in train_loader:\n",
    "  x=x.type(torch.float32)\n",
    "  encoder(x.to(device))\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "YUNyachcDL2a"
   },
   "outputs": [],
   "source": [
    "class GPT2ConditionedOnEncoder(nn.Module):\n",
    "    def __init__(self, encoder, hidden_size=768):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\",config=gptconfig)\n",
    "        self.gpt2.resize_token_embeddings(self.gpt2.config.vocab_size + 2)  # in case of custom tokenizer\n",
    "        self.projection = nn.Linear(hidden_size, self.gpt2.config.n_embd)\n",
    "\n",
    "    def forward(self, landmarks, input_ids, attention_mask,labels=None,proj=False):\n",
    "\n",
    "        encoder_output, (h_n, _) = self.encoder(landmarks)  # h_n: [num_layers, 1, hidden_size]\n",
    "        h_n = h_n[-1]  # use top layer [1, hidden_size]\n",
    "        projected = self.projection(h_n)  # [1, emb_dim]\n",
    "        projected = projected.unsqueeze(1)  # [1, 1, emb_dim]\n",
    "\n",
    "        outputs = self.gpt2(input_ids=input_ids, encoder_hidden_state=encoder_output, labels=labels) # Pass encoder _hiddent to \n",
    "        return outputs.logits,outputs.loss,projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "isY_WHUuDR9d",
    "outputId": "c6c629a5-bddf-40bc-9570-b1b710482197"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = GPT2ConditionedOnEncoder(encoder,hidden_size=config.hidden_size)\n",
    "model.to(device)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "fKwfuE1dQQX3"
   },
   "outputs": [],
   "source": [
    "from helper_functions import progress_bar, plot_loss_curves,SaveModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "53P_V5elQIw0"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "save_model_checkpoint = SaveModelCheckpoint(path=\"asl_lens_model_gpt2_checkpoint_hidden1.pt\")\n",
    "\n",
    "best_val_loss=float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_knpeGPqNct",
    "outputId": "0b8fc29b-bbd6-44b9-f9df-85a446c2a6ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Batch 370/374 - [====================================================]\n",
      "\u001b[92m\u001b[1mModel saved at epoch: 1, val_loss improved from: inf to: 3.5965\u001b[0m\n",
      "Epoch 1/20, Train loss: 3.6605, Val loss: 3.5965, Duration: 0:07:07.233264\n",
      "-------------------------------------------------------------\n",
      "Validation: Batch 370/374 - [====================================================]\n",
      "Epoch 2/20, Train loss: 3.3403, Val loss: 3.6906, Duration: 0:04:28.839368\n",
      "-------------------------------------------------------------\n",
      "Validation: Batch 370/374 - [====================================================]\n",
      "Epoch 3/20, Train loss: nan, Val loss: nan, Duration: 0:06:06.913859\n",
      "-------------------------------------------------------------\n",
      "Batch 60/1820 - [=.................................................]"
     ]
    }
   ],
   "source": [
    "epoches=20\n",
    "train_losses = np.zeros(epoches)\n",
    "val_losses = np.zeros(epoches)\n",
    "for it in range(epoches):\n",
    "  t0 = datetime.now()\n",
    "  current_batch = 0\n",
    "  total_batches = len(train_loader)\n",
    "  model.train()\n",
    "  train_loss=[]\n",
    "  val_loss=[]\n",
    "  hidden_state = None\n",
    "  # train\n",
    "  for data,x,mask,targets in train_loader:\n",
    "    #break\n",
    "    # move data to gpu\n",
    "    #inputs,targets = (inputs[0].to(device),inputs[1].to(device),inputs[2].to(device)),targets.to(device)\n",
    "    #inputs = inputs.permute(0,2,1)\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    # forward pass\n",
    "    _,loss,_=model(data.to(device),x.to(device),mask.to(device),labels=targets.to(device))\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss.append(loss.item())\n",
    "    current_batch = progress_bar(current_batch,total_batches)\n",
    "\n",
    "  model.eval()\n",
    "  current_batch = 0\n",
    "  total_batches = len(test_loader)\n",
    "  for data,x,mask,targets in test_loader:\n",
    "    # move data to gpu\n",
    "    _,loss,_=model(data.to(device),x.to(device),mask.to(device),labels=targets.to(device))\n",
    "    val_loss.append(loss.item())\n",
    "    current_batch = progress_bar(current_batch,total_batches,validation=True)\n",
    "\n",
    "\n",
    "  # calculate loss\n",
    "  print('\\r')\n",
    "  train_loss = np.mean(train_loss)\n",
    "  val_loss = np.mean(val_loss)\n",
    "  best_val_loss=  save_model_checkpoint(val_loss,best_val_loss,train_loss,it, model=model, optimizer=optimizer)\n",
    "  # append loss\n",
    "  train_losses[it]=train_loss\n",
    "  val_losses[it]=val_loss\n",
    "  dt = datetime.now() - t0\n",
    "  print(f\"Epoch {it+1}/{epoches}, Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}, Duration: {dt}\")\n",
    "  print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pv1wjCzJXUBN"
   },
   "outputs": [],
   "source": [
    "\n",
    "file=h5py.File(f\"landmarks/g0yUlOaqL6k.h5\")\n",
    "start_frame=0\n",
    "end_frame=100\n",
    "hand_left=file[\"handLeft\"][start_frame:end_frame]\n",
    "# print(self.asl_df[\"frames\"][index])\n",
    "# print(start_frame,end_frame)\n",
    "hand_right=file[\"handRight\"][start_frame:end_frame]\n",
    "face_lips=file[\"faceLips\"][start_frame:end_frame]\n",
    "face_oval=file[\"faceOval\"][start_frame:end_frame]\n",
    "file.close()\n",
    "#x = self.text[idx:idx + self.seq_len]\n",
    "x=np.concatenate([hand_left,hand_right,face_lips,face_oval],axis=1)\n",
    "test= torch.tensor(x).long()\n",
    "test=test.type(torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8dU8cpCORNTk",
    "outputId": "a0f6db8d-0063-45f9-c229-5117c2dd4b8f"
   },
   "outputs": [],
   "source": [
    "data,l,m,b = train_asl_dataset.__getitem__(3)\n",
    "#data,y = (data[0].to(device),data[1].to(device),data[2].to(device)),y.to(device)\n",
    "# Suppose <SOS> is a special token you've added, or just a regular token\n",
    "#data,k,l = data[0].unsqueeze(0),(l[0].unsqueeze(0),l[1].unsqueeze(0)),b.unsqueeze(0)\n",
    "decoded_text=tokenizer.eos_token\n",
    "for i in range(1):\n",
    "  #inputs = tokenizer(tokenizer.decode(k[0][0], skip_special_tokens=True), return_tensors=\"pt\", padding=\"max_length\", max_length=40, truncation=True).to(device)#data[1]#torch.tensor([[tokenizer.bos_token_id]]).to(device)  # shape: [1, 1]\n",
    "  #input_ids = x2[0].to(device).long()\n",
    "  #mask = x2[1].to(device).long()\n",
    "  #y=ys.to(device).long()\n",
    "  print(data[0][0])\n",
    "  data,x,m,y = train_asl_dataset.__getitem__(3)\n",
    "  outputs,loss,_=model(data.to(device),x.to(device), m.to(device),labels=y.to(device))\n",
    "  predicted_token_ids = torch.argmax(outputs, dim=-1)  # shape: [batch_size, seq_len]\n",
    "  print(predicted_token_ids)\n",
    "  print(torch.isnan(m).any())\n",
    "  # Example: remove the first token from each sequence (e.g., your conditioning token)\n",
    "  #predicted_token_ids = predicted_token_ids[:, -1]\n",
    "  #print(predicted_token_ids)\n",
    "  #input_ids = torch.cat((x.to(device),predicted_token_ids),dim=1)\n",
    "  #decoded_text += tokenizer.decode(predicted_token_ids, skip_special_tokens=True)\n",
    "  #break\n",
    "print(decoded_text+tokenizer.decode(predicted_token_ids[0], skip_special_tokens=True))\n",
    "print(loss)#print(predicted_token_ids)\n",
    "#print(b)\n",
    "#print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "_YovzYW7WAY9",
    "outputId": "907396a4-a808-47df-f11e-1dcf4d57136e"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(k[0][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 802
    },
    "id": "Uut_BgMLZRg0",
    "outputId": "7d336d0c-243e-452c-d15a-5bf445650f09"
   },
   "outputs": [],
   "source": [
    "df[df['VIDEO_ID']==\"g0yUlOaqL6k\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vdh5RBldOGK_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

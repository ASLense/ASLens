{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6950eade-5f41-445b-8d25-dd05535f4daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import mediapipe as mp\n",
    "from constants import LIPS_POSITIONS, FACE_OVAL,HAND_POSITIONS,HAND_CONNECTIONS\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from mediapipe.tasks.python.components.containers import NormalizedLandmark\n",
    "\n",
    "DEFAULT_HAND = np.load(\"defaultHand.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7373f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Drawing:\n",
    "    def __init__(self):\n",
    "        self.mpFace = mp.solutions.face_mesh\n",
    "        self.mpDrawHands = mp.solutions.drawing_utils # Initializing drawing object for hands\n",
    "        self.mpDrawFace = mp.solutions.drawing_utils # Initializing drawing object for Face\n",
    "        self.mp_drawing_styles =mp.solutions.drawing_styles\n",
    "        self.mp_drawing_face = self.mpDrawFace.DrawingSpec(color=(0,0,200),thickness=0,circle_radius=1) #Initializing drawing specifications for face\n",
    "        self.mp_drawing_hands = self.mpDrawHands.DrawingSpec(color=(255,0,0),thickness=0,circle_radius=1) #Initializing drawing specifications for hand\n",
    "        self.mpHands = mp.solutions.hands\n",
    "    def drawLandmarks(self,img,resultsFace,resultsHands,img_size=(700,720)):\n",
    "        img=img.copy()\n",
    "        colors={\"Right\":(100,100,100),\"Left\":(0,0,255)}\n",
    "        if resultsFace is not None:\n",
    "            for var in resultsFace:\n",
    "                cv2.circle(img, (int(var[0]*img_size[0]),int(var[1]*img_size[0])), 1, (0, 0, 255), -1)\n",
    "        for key in resultsHands:\n",
    "            points={}\n",
    "            for i,var in enumerate(resultsHands[key]):\n",
    "                point = (int(var[0]*img_size[0]),int(var[1]*img_size[0]))\n",
    "                cv2.circle(img, point, 3, colors[key], -1)\n",
    "                points[i]=point\n",
    "            for conn in HAND_CONNECTIONS:\n",
    "                cv2.line(img, points[conn], points[HAND_CONNECTIONS[conn]], (216, 223, 230), 2)\n",
    "        return img\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "790b3d25-cf0b-4d46-93d1-04e812b15457",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkExtractor:\n",
    "    def __init__(self):\n",
    "        self.mpHands = mp.solutions.hands # Load mediapipe hands module\n",
    "        self.mpFace = mp.solutions.face_mesh\n",
    "        self.hands = self.mpHands.Hands( # Initialize hands model\n",
    "            max_num_hands=2,\n",
    "            model_complexity=1,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5,\n",
    "            static_image_mode=False)\n",
    "        \n",
    "         # Load mediapipe face module\n",
    "        self.faces = self.mpFace.FaceMesh( # Initialize Face model\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5,\n",
    "            static_image_mode=False)\n",
    "    def getHandLandmarks(self,hand,scale=False,img_size=(700,720)):\n",
    "        list_hand_positions=[]\n",
    "       # print(type(resultsFace.multi_face_landmarks[0]))\n",
    "\n",
    "        for cord in HAND_POSITIONS:\n",
    "            x1,y1,z1=self.__getCoordinates(hand,cord,scale,img_size)\n",
    "\n",
    "            list_hand_positions.append((x1,y1,z1))\n",
    "        return np.array(list_hand_positions)    \n",
    "    def findHands(self,img,resultsFace):\n",
    "        hands={}\n",
    "        imgRGB = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) # Transform to RGB\n",
    "        results = self.hands.process(imgRGB) # Feeding image through Hands model\n",
    "        #print(resultsFace.multi_face_landmarks[0])\n",
    "        #face = resultsFace.multi_face_landmarks[0]\n",
    "        if results.multi_handedness!=None:\n",
    "            for i,hand in enumerate(results.multi_handedness):\n",
    "                if hand.classification[0].label == \"Left\":\n",
    "                    handType=\"Right\"\n",
    "                else:\n",
    "                    handType=\"Left\"\n",
    "                hands[handType]=results.multi_hand_landmarks[i]\n",
    "\n",
    "        for key in hands:\n",
    "            hands[key]=self.getHandLandmarks(hands[key])\n",
    "        return hands # Returning values from model prediction\n",
    "        \n",
    "    def findFace(self, img):\n",
    "        imgRGB = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) # Transform image to RGB\n",
    "        results = self.faces.process(imgRGB) # Feeding image through Face model\n",
    "        return results # Returning values from model prediction\n",
    "        \n",
    "    \n",
    "    def __getCoordinates(self,landmarks,index,scale,img_size): \n",
    "        x=landmarks.landmark[index].x\n",
    "        y=landmarks.landmark[index].y\n",
    "        z=landmarks.landmark[index].z\n",
    "        if scale:\n",
    "            x=x*img_size[0]\n",
    "            y=y*img_size[1]\n",
    "        return x,y,z  \n",
    "        \n",
    "    def getLipsLandmarks(self,resultsFace,scale=False,img_size=(700,720)):\n",
    "        list_lips_positions=[]\n",
    "        if resultsFace.multi_face_landmarks:\n",
    "            landmarkovi=resultsFace.multi_face_landmarks[0]\n",
    "\n",
    "            for cord in LIPS_POSITIONS:\n",
    "                x1,y1,z1=self.__getCoordinates(landmarkovi,cord[0],scale,img_size)\n",
    "                x2,y2,z2=self.__getCoordinates(landmarkovi,cord[1],scale,img_size)\n",
    "\n",
    "                avg_x=float((x1+x2)/2)\n",
    "                avg_y=float((y1+y2)/2)\n",
    "\n",
    "                list_lips_positions.append((avg_x,avg_y,z1))\n",
    "        return np.array(list_lips_positions)\n",
    " \n",
    "    def getFaceLandmarks(self,resultsFace,scale=False,img_size=(700,720)):\n",
    "        list_face_positions=[]\n",
    "       # print(type(resultsFace.multi_face_landmarks[0]))\n",
    "        if resultsFace.multi_face_landmarks:\n",
    "            landmarkovi=resultsFace.multi_face_landmarks[0]\n",
    "\n",
    "            for cord in FACE_OVAL:\n",
    "                x1,y1,z1=self.__getCoordinates(landmarkovi,cord,scale,img_size)\n",
    "\n",
    "                list_face_positions.append((x1,y1,z1))\n",
    "        return np.array(list_face_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e77eb362-db4d-4ae9-97c9-3b94d2ad395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoLoader:\n",
    "    def __init__(self):\n",
    "        self.fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        self.landmark_extractor=LandmarkExtractor()\n",
    "        self.drawing = Drawing()\n",
    "\n",
    "    def loadVideo(self,path,output_path=None):\n",
    "        \n",
    "        cap = cv2.VideoCapture(path)\n",
    "        if output_path is not None:\n",
    "            out = cv2.VideoWriter(output_path,self.fourcc, 15,(700,720))\n",
    "\n",
    "        use_frame=True\n",
    "        frames=[]  \n",
    "        i = 0\n",
    "        while(True):\n",
    "            ret, frame = cap.read() #reading frames\n",
    "            if ret: #if frame exist ret=True, otherwise False\n",
    "                if use_frame: # this means we will skip every other frame\n",
    "                    frame=frame[:, 300:1000,:] #cropping image, retainig all 3 rgb channels\n",
    "                    frames.append(frame)\n",
    "                    i+=1\n",
    "                   # print((i*(1000/15))/1000)\n",
    "                    resultsFace=self.landmark_extractor.findFace(frame) #using function defined above to detect facial landmarks in a frame (findFace)\n",
    "                    resultsFace=self.landmark_extractor.getFaceLandmarks(resultsFace)\n",
    "                    resultsHands=self.landmark_extractor.findHands(frame,resultsFace) #using function defined above to detect hand landmarks in a frame (findHnds)\n",
    "                    if output_path is not None:\n",
    "                        out.write(self.drawing.drawLandmarks(frame.copy(),resultsFace,resultsHands)) #drawing landmarks on frames by using function defined above (drawLadmarks)\n",
    "        \n",
    "                    use_frame=False\n",
    "                else:\n",
    "                    use_frame=True\n",
    "            else:\n",
    "                break\n",
    "        if output_path is not None:\n",
    "            out.release() #close writing stream\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00bb0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "videoLoader = VideoLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6968f12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frames=videoLoader.loadVideo(\"../../ASLens - test data 1/-g45vqccdzI-1-rgb_front.mp4\",output_path='novitest41.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a08893e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r=15\n",
    "landmark_extractor=LandmarkExtractor()\n",
    "resultsFace=landmark_extractor.findFace(frames[r]) #using function defined above to detect facial landmarks in a frame (findFace)\n",
    "resultsFace=landmark_extractor.getFaceLandmarks(resultsFace)\n",
    "resultsHands=landmark_extractor.findHands(frames[r],resultsFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e1c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4f9c192",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resultsFace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m (resultsFace[:,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39mresultsFace[:,\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'resultsFace' is not defined"
     ]
    }
   ],
   "source": [
    "(resultsFace[:,0]+resultsFace[:,1])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "24a63f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tada ={1:3,2:3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "4e160cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in tada:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde3e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

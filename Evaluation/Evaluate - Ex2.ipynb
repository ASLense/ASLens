{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca55afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import string\n",
    "import h5py\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f0d90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b557ab4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=1536, nx=768)\n",
       "          (q_attn): Conv1D(nf=768, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel,GPT2Config\n",
    "\n",
    "gptconfig = GPT2Config.from_pretrained(\"gpt2\",add_cross_attention=True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token\n",
    "#tokenizer.add_special_tokens({\"bos_token\":\"<s>\",\"eos_token\":\"</s>\"})\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\",config=gptconfig)\n",
    "gpt2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ca3930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name       g0yUlOaqL6k\n",
      "landmarks    g0yUlOaqL6k.h5\n",
      "frames                 2026\n",
      "sentences                24\n",
      "Name: 121, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>landmarks</th>\n",
       "      <th>frames</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FzmL8SL6Bow</td>\n",
       "      <td>FzmL8SL6Bow.h5</td>\n",
       "      <td>1196</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FZrU_mEryAs</td>\n",
       "      <td>FZrU_mEryAs.h5</td>\n",
       "      <td>1213</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-g45vqccdzI</td>\n",
       "      <td>-g45vqccdzI.h5</td>\n",
       "      <td>1332</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FzUdcaxw_vs</td>\n",
       "      <td>FzUdcaxw_vs.h5</td>\n",
       "      <td>1826</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-g0iPSnQt6w</td>\n",
       "      <td>-g0iPSnQt6w.h5</td>\n",
       "      <td>1657</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     file_name       landmarks  frames  sentences\n",
       "0  FzmL8SL6Bow  FzmL8SL6Bow.h5    1196          4\n",
       "1  FZrU_mEryAs  FZrU_mEryAs.h5    1213          7\n",
       "2  -g45vqccdzI  -g45vqccdzI.h5    1332         10\n",
       "3  FzUdcaxw_vs  FzUdcaxw_vs.h5    1826         19\n",
       "4  -g0iPSnQt6w  -g0iPSnQt6w.h5    1657         17"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"how2sign.csv\",sep=\"\\t\")\n",
    "df.tail()\n",
    "asl_df=pd.read_csv(\"ASLens-landmarks.csv\")\n",
    "print(asl_df.iloc[121])\n",
    "asl_df.drop(121,inplace=True)\n",
    "asl_df.index = np.arange(0,len(asl_df))\n",
    "#asl_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "asl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3aebe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASLensDataset(Dataset):\n",
    "  def __init__(self, df, asl_df, tokenizer=None, seq_len=128):\n",
    "    self.tokenizer=tokenizer\n",
    "    self.df=df\n",
    "    self.asl_df=asl_df\n",
    "    self.seq_len=seq_len\n",
    "  def __len__(self):\n",
    "    return self.asl_df['sentences'].sum()\n",
    "\n",
    "  def extract_number(self,sentence_id):\n",
    "    # Extract the numeric part after the last underscore\n",
    "    match = re.search(r'_(\\d+)$', sentence_id)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "  def extractFrames(self,ex,index):\n",
    "    fName=self.asl_df[\"landmarks\"][index]\n",
    "    file=h5py.File(f\"landmarks/{fName}\")\n",
    "    start_frame=int(ex[\"START_REALIGNED\"]*15)\n",
    "    end_frame=int(ex[\"END_REALIGNED\"]*15)\n",
    "    hand_left=file[\"handLeft\"][start_frame:end_frame]\n",
    "    # print(self.asl_df[\"frames\"][index])\n",
    "    # print(start_frame,end_frame)\n",
    "    hand_right=file[\"handRight\"][start_frame:end_frame]\n",
    "    face_lips=file[\"faceLips\"][start_frame:end_frame]\n",
    "    face_oval=file[\"faceOval\"][start_frame:end_frame]\n",
    "    file.close()\n",
    "    #x = self.text[idx:idx + self.seq_len]\n",
    "    x=np.concatenate([hand_left,hand_right,face_lips,face_oval],axis=1)\n",
    "    return torch.tensor(x)\n",
    "  def __getitem__ (self, idx):\n",
    "    index=0\n",
    "    while idx>self.asl_df['sentences'][index]-1:\n",
    "      idx-=self.asl_df['sentences'][index]\n",
    "      index+=1\n",
    "    file_name=self.asl_df[\"file_name\"][index]\n",
    "\n",
    "    if file_name[-1]==\"-\":\n",
    "\n",
    "      file_name=file_name[:-1]\n",
    "    sent=df[df['VIDEO_ID']==file_name]\n",
    "    sent = sent.copy()  # Explicit copy\n",
    "    sent['SENTENCE_NUM'] = sent['SENTENCE_ID'].apply(self.extract_number)\n",
    "    sent = sent.sort_values([\"VIDEO_NAME\",\"SENTENCE_NUM\"])\n",
    "\n",
    "    if len(sent[sent.duplicated(\"SENTENCE_ID\")])>0:\n",
    "      first = sent.drop_duplicates(subset=\"SENTENCE_NUM\",keep=\"first\")\n",
    "      last = sent.drop_duplicates(subset=\"SENTENCE_NUM\",keep=\"last\")\n",
    "      numOfFrames=self.asl_df[\"frames\"][index]\n",
    "      if numOfFrames>first[\"END_REALIGNED\"].max()*15:\n",
    "        sent = first\n",
    "      else:\n",
    "        sent = last\n",
    "    #print(sent)\n",
    "    ex=sent.iloc[idx]\n",
    "\n",
    "    frames=self.extractFrames(ex,index)\n",
    "    frames=frames.type(torch.float32)\n",
    "    text=ex[\"SENTENCE\"]+tokenizer.eos_token\n",
    "    \n",
    "    if self.tokenizer:\n",
    "        inputs = self.tokenizer(text[:int(len(text)*0.33)], return_tensors=\"pt\", padding=\"max_length\", max_length=20, truncation=True,add_special_tokens=True)\n",
    "        labels = self.tokenizer(text[int(len(text)*0.33):], return_tensors=\"pt\", padding=\"max_length\", max_length=20, truncation=True).input_ids\n",
    "    return frames, text[:int(len(text)*0.33)], text[int(len(text)*0.33):]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8ba9d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.15\n",
    "test_df = asl_df.iloc[int(len(asl_df)*(1-test_size)):]\n",
    "test_df.index = np.arange(0,len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e592720",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_asl_dataset = ASLensDataset(df,test_df,tokenizer=None)\n",
    "test_loader = DataLoader(test_asl_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae0063a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    hidden_size: int\n",
    "    num_layers: int\n",
    "    dropout_rate:  float =0.1\n",
    "    learning_rate:float= 0.001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0a663d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ASLensEncoder(nn.Module):\n",
    "  def __init__(self,config):\n",
    "    super(ASLensEncoder,self).__init__()\n",
    "    self.config=config\n",
    "    self.conv1 = nn.Sequential(\n",
    "        #nn.Conv2d(1,16,kernel_size=(3,1),padding=(1,0)),\n",
    "        #nn.ReLU(),\n",
    "\n",
    "       # nn.MaxPool2d(kernel_size=(2,2)),\n",
    "        #nn.Conv2d(16,32,kernel_size=(2,1),padding=(1,0)),\n",
    "       # nn.ReLU(),\n",
    "        #nn.Conv2d(64,128,kernel_size=(3,3)),\n",
    "        #nn.ReLU(),\n",
    "        #nn.Flatten()\n",
    "      nn.Conv1d(3, 16, kernel_size=3, padding=1),  # preserves (90, 3)\n",
    "      nn.ReLU(),\n",
    "      nn.Conv1d(16, 32, kernel_size=2,padding=1),                 # reduces width\n",
    "      nn.ReLU(),\n",
    "      nn.Conv1d(32, 64, kernel_size=2,padding=1),                 # reduces width\n",
    "      nn.ReLU(),\n",
    "      #nn.AdaptiveAvgPool2d((45, 3))\n",
    "        #nn.Conv3d(128,64,kernel_size=(3,3,1)),\n",
    "        #nn.ReLU(),\n",
    "        )\n",
    "    self.lstm= nn.LSTM(input_size=98,\n",
    "                       hidden_size=self.config.hidden_size,\n",
    "                       num_layers=config.num_layers,\n",
    "                       dropout=config.dropout_rate,\n",
    "                       batch_first=True)\n",
    "\n",
    "\n",
    "  def forward(self,x):\n",
    "    #x = x.permute(0, 3, 1, 2)  # [32, 3, 100, 90]\n",
    "    #x = x.unsqueeze(-1)\n",
    "    time = x.shape[2]\n",
    "    x=x.view(-1, 98,3)\n",
    "    x=x.permute(1,2, 0)\n",
    "    out = self.conv1(x)\n",
    "    #out,_ = torch.max(out,2)\n",
    "    #out = out.view(out.shape[0],-1)\n",
    "   # return out\n",
    "    out=out.reshape(1, -1, 98).contiguous()\n",
    "    out,hidden = self.lstm(out)\n",
    "    return out,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5acb4c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ASLensEncoder(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv1d(3, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): Conv1d(16, 32, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "    (3): ReLU()\n",
       "    (4): Conv1d(32, 64, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (lstm): LSTM(98, 128, num_layers=2, batch_first=True, dropout=0.2)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ModelConfig(hidden_size=128,num_layers=2,dropout_rate=0.2)\n",
    "encoder = ASLensEncoder(config)\n",
    "encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54519066",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2ConditionedOnEncoder(nn.Module):\n",
    "    def __init__(self, encoder, hidden_size=768):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\",config=gptconfig)\n",
    "        self.gpt2.resize_token_embeddings(self.gpt2.config.vocab_size + 2)  # in case of custom tokenizer\n",
    "        self.projection = nn.Linear(hidden_size, self.gpt2.config.n_embd)\n",
    "\n",
    "    def forward(self, landmarks, input_ids, attention_mask,labels=None,proj=False):\n",
    "        # frames: [1, seq_len, 98, 3] or similar\n",
    "        #if proj == False:\n",
    "        encoder_output, (h_n, _) = self.encoder(landmarks)  # h_n: [num_layers, 1, hidden_size]\n",
    "        h_n = h_n[-1]  # use top layer [1, hidden_size]\n",
    "        projected = self.projection(h_n)  # [1, emb_dim]\n",
    "        projected = projected.unsqueeze(1)  # [1, 1, emb_dim]\n",
    "       # else:\n",
    "         # projected=landmarks\n",
    "        # Get input embeddings\n",
    "        #input_embeds = self.gpt2.transformer.wte(input_ids)  # [1, seq_len, emb_dim]\n",
    "        #input_embeds = torch.cat([projected, input_embeds], dim=1)  # [1, seq_len+1, emb_dim]\n",
    "       # attention_mask = torch.cat([\n",
    "       #     torch.ones((input_ids.size(0), 1), dtype=torch.long, device=input_ids.device),\n",
    "        #    attention_mask\n",
    "       # ], dim=1)\n",
    "        # Assume labels: [batch, seq_len]\n",
    "        #if labels is not None:\n",
    "        #  ignore_label = torch.full((labels.size(0),1), -100, dtype=labels.dtype, device=labels.device)\n",
    "        #  labels = torch.cat([ignore_label, labels], dim=1)\n",
    "        #print(torch.isnan(encoder_output).any())\n",
    "        outputs = self.gpt2(input_ids=input_ids, encoder_hidden_state=encoder_output, labels=labels)\n",
    "        return outputs.logits,outputs.loss,projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a44916a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2ConditionedOnEncoder(encoder,hidden_size=config.hidden_size)\n",
    "model.to(device)\n",
    "print()\n",
    "model.load_state_dict(torch.load('asl_lens_model_gpt2_checkpoint_hidden1.pt',map_location=device,weights_only=False)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8d3fdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jiwer in /opt/conda/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from jiwer) (8.2.1)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /opt/conda/lib/python3.10/site-packages (from jiwer) (3.13.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8af8735",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from mosestokenizer import MosesTokenizer\n",
    "from mosestokenizer import MosesDetokenizer\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "from rouge_score import rouge_scorer\n",
    "from jiwer import wer  # Install with: pip install jiwer\n",
    "\n",
    "# Download NLTK data (run once)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def compute_metrics(reference, hypothesis):\n",
    "    # Tokenize sentences\n",
    "    tokenize = MosesTokenizer('en')\n",
    "    detokenize = MosesDetokenizer('en')\n",
    "    \n",
    "    ref_tokens = tokenize(reference)\n",
    "    hyp_tokens = tokenize(hypothesis)\n",
    "    \n",
    "    # Compute BLEU (with smoothing)\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_score = sentence_bleu(\n",
    "        [ref_tokens],\n",
    "        hyp_tokens,\n",
    "        smoothing_function=smoothing\n",
    "    )\n",
    "    \n",
    "    # Compute METEOR\n",
    "    meteor_score_val = meteor_score(\n",
    "        [ref_tokens],\n",
    "        hyp_tokens\n",
    "    )\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(reference, hypothesis)\n",
    "    \n",
    "    return {\n",
    "        \"BLEU\": bleu_score,\n",
    "        \"METEOR\": meteor_score_val,\n",
    "        \"ROUGE-1\": rouge_scores['rouge1'].fmeasure,\n",
    "        \"ROUGE-2\": rouge_scores['rouge2'].fmeasure,\n",
    "        \"ROUGE-L\": rouge_scores['rougeL'].fmeasure,\n",
    "        \"WER\": wer(reference, hypothesis)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27e2a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19e566f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=[]\n",
    "for i in range(len(test_asl_dataset)):\n",
    "    data, x, y = test_asl_dataset.__getitem__(i)\n",
    "    data = data.to(device)  # Move data to device first\n",
    "    \n",
    "    decoded_text = x\n",
    "    for i in range(len(y)):\n",
    "        inputs = tokenizer(decoded_text, return_tensors=\"pt\").to(device)\n",
    "        input_ids = inputs.input_ids.to(device)\n",
    "        mask = inputs.attention_mask.to(device)\n",
    "        \n",
    "        # All operations now on device\n",
    "        encoder_output, (h_n, _) = model.encoder(data)  # data is already on device\n",
    "        h_n = h_n[-1]\n",
    "        projected = model.projection(h_n)  # No need to move to device again\n",
    "        projected = projected.unsqueeze(1)\n",
    "        \n",
    "        # Ensure all inputs are on same device\n",
    "        t = gpt2.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=mask,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            encoder_hidden_states=projected,\n",
    "            max_length=input_ids.shape[1]+1\n",
    "        )\n",
    "        \n",
    "        decoded_text = tokenizer.decode(t[0], skip_special_tokens=True)\n",
    "    \n",
    "    metrics.append(compute_metrics(y, decoded_text.replace(\"\\n\", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d897c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "b={'BLEU': 0.,\n",
    "  'METEOR': 0.,\n",
    "  'ROUGE-1': 0.,\n",
    "  'ROUGE-2': 0.0,\n",
    "  'ROUGE-L': 0.,\n",
    "  'WER': 0.}\n",
    "for k in metrics:\n",
    "    b[\"BLEU\"]+=k[\"BLEU\"]\n",
    "    b[\"METEOR\"]+=k[\"METEOR\"]\n",
    "    b[\"ROUGE-1\"]+=k[\"ROUGE-1\"]\n",
    "    b[\"ROUGE-2\"]+=k[\"ROUGE-2\"]\n",
    "    b[\"ROUGE-L\"]+=k[\"ROUGE-L\"]\n",
    "    b[\"WER\"]+=k[\"WER\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e2bdd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "b[\"BLEU\"]/=len(test_asl_dataset)\n",
    "b[\"METEOR\"]/=len(test_asl_dataset)\n",
    "b[\"ROUGE-1\"]/=len(test_asl_dataset)\n",
    "b[\"ROUGE-2\"]/=len(test_asl_dataset)\n",
    "b[\"ROUGE-L\"]/=len(test_asl_dataset)\n",
    "b[\"WER\"]/=len(test_asl_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19d1feea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.barplot(x=[1,2],y=[0.01168,0.01389])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c880d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
